# ç¾èµ›æœºå™¨å­¦ä¹ ç®—æ³•æ¨¡æ¿

## é¡¹ç›®ç®€ä»‹

æœ¬é¡¹ç›®ä¸ºç¾å›½å¤§å­¦ç”Ÿæ•°å­¦å»ºæ¨¡ç«èµ›ï¼ˆMCM/ICMï¼‰çš„C/E/Fé¢˜æä¾›å¸¸ç”¨æœºå™¨å­¦ä¹ ç®—æ³•æ¨¡æ¿ï¼ŒåŒ…æ‹¬å¤šå…ƒçº¿æ€§å›å½’ã€éšæœºæ£®æ—ã€é›†æˆå­¦ä¹ ç­‰å¤šç§ç®—æ³•çš„å®ç°å’Œä½¿ç”¨ç¤ºä¾‹ã€‚

## ç›®å½•ç»“æ„

```
â”œâ”€â”€ algorithms/              # ç®—æ³•æ¨¡å—
â”‚   â”œâ”€â”€ regression/         # å›å½’ç®—æ³•
â”‚   â”‚   â””â”€â”€ multiple_linear_regression.py
â”‚   â”œâ”€â”€ classification/     # åˆ†ç±»ç®—æ³•
â”‚   â”‚   â”œâ”€â”€ logistic_regression.py
â”‚   â”‚   â”œâ”€â”€ svm.py
â”‚   â”‚   â”œâ”€â”€ knn.py
â”‚   â”‚   â””â”€â”€ decision_tree.py
â”‚   â”œâ”€â”€ ensemble/          # é›†æˆå­¦ä¹ ç®—æ³•
â”‚   â”‚   â”œâ”€â”€ random_forest.py
â”‚   â”‚   â””â”€â”€ ensemble_methods.py
â”‚   â””â”€â”€ utils/             # å·¥å…·æ¨¡å—
â”‚       â”œâ”€â”€ preprocessing.py
â”‚       â””â”€â”€ evaluation.py
â”œâ”€â”€ examples/              # ä½¿ç”¨ç¤ºä¾‹
â”œâ”€â”€ data/                  # æ•°æ®æ–‡ä»¶å¤¹
â”œâ”€â”€ requirements.txt       # ä¾èµ–åŒ…
â””â”€â”€ README.md             # é¡¹ç›®è¯´æ˜
```

## å®‰è£…ä¾èµ–

```bash
pip install -r requirements.txt
```

## ç®—æ³•åˆ—è¡¨

### å›å½’ç®—æ³•

1. **å¤šå…ƒçº¿æ€§å›å½’** (`algorithms/regression/multiple_linear_regression.py`)
   - é€‚ç”¨äºè¿ç»­å˜é‡é¢„æµ‹
   - æ”¯æŒç‰¹å¾æ ‡å‡†åŒ–
   - æä¾›ç³»æ•°è§£é‡Šå’Œå¯è§†åŒ–

### åˆ†ç±»ç®—æ³•

2. **é€»è¾‘å›å½’** (`algorithms/classification/logistic_regression.py`)
   - äºŒåˆ†ç±»å’Œå¤šåˆ†ç±»æ”¯æŒ
   - æ”¯æŒL1/L2æ­£åˆ™åŒ–
   - æä¾›ROCæ›²çº¿å’Œæ··æ·†çŸ©é˜µ

3. **æ”¯æŒå‘é‡æœº (SVM)** (`algorithms/classification/svm.py`)
   - æ”¯æŒå¤šç§æ ¸å‡½æ•°ï¼ˆçº¿æ€§ã€RBFã€å¤šé¡¹å¼ï¼‰
   - å›å½’å’Œåˆ†ç±»ä»»åŠ¡
   - è¶…å‚æ•°è‡ªåŠ¨è°ƒä¼˜

4. **Kè¿‘é‚» (KNN)** (`algorithms/classification/knn.py`)
   - ç®€å•é«˜æ•ˆçš„åˆ†ç±»å›å½’ç®—æ³•
   - è‡ªåŠ¨å¯»æ‰¾æœ€ä¼˜Kå€¼
   - æ”¯æŒå¤šç§è·ç¦»åº¦é‡

5. **å†³ç­–æ ‘** (`algorithms/classification/decision_tree.py`)
   - å¯è§£é‡Šæ€§å¼º
   - æ ‘ç»“æ„å¯è§†åŒ–
   - ç‰¹å¾é‡è¦æ€§åˆ†æ

### é›†æˆå­¦ä¹ ç®—æ³•

6. **éšæœºæ£®æ—** (`algorithms/ensemble/random_forest.py`)
   - å›å½’å’Œåˆ†ç±»ä»»åŠ¡
   - ç‰¹å¾é‡è¦æ€§æ’åº
   - è¶…å‚æ•°è°ƒä¼˜

7. **æ¢¯åº¦æå‡ (Gradient Boosting)** (`algorithms/ensemble/ensemble_methods.py`)
   - é«˜ç²¾åº¦é¢„æµ‹
   - æ”¯æŒå›å½’å’Œåˆ†ç±»
   - ç‰¹å¾é‡è¦æ€§åˆ†æ

8. **AdaBoost** (`algorithms/ensemble/ensemble_methods.py`)
   - è‡ªé€‚åº”æå‡ç®—æ³•
   - å‡å°‘åå·®å’Œæ–¹å·®
   - é€‚åˆä¸­å°è§„æ¨¡æ•°æ®é›†

9. **XGBoost** (`algorithms/ensemble/ensemble_methods.py`)
   - é«˜æ€§èƒ½æ¢¯åº¦æå‡
   - ç«èµ›å¸¸ç”¨ç®—æ³•
   - ä¸°å¯Œçš„è°ƒå‚é€‰é¡¹

10. **æŠ•ç¥¨é›†æˆ (Voting Ensemble)** (`algorithms/ensemble/ensemble_methods.py`)
    - ç»“åˆå¤šä¸ªæ¨¡å‹é¢„æµ‹
    - ç¡¬æŠ•ç¥¨å’Œè½¯æŠ•ç¥¨
    - æé«˜æ¨¡å‹ç¨³å®šæ€§

11. **å †å é›†æˆ (Stacking)** (`algorithms/ensemble/ensemble_methods.py`)
    - å¤šå±‚æ¨¡å‹ç»„åˆ
    - å…ƒå­¦ä¹ å™¨ä¼˜åŒ–
    - æœ€å¤§åŒ–æ¨¡å‹æ€§èƒ½

### å·¥å…·æ¨¡å—

12. **æ•°æ®é¢„å¤„ç†** (`algorithms/utils/preprocessing.py`)
    - ç¼ºå¤±å€¼å¤„ç†
    - ç‰¹å¾ç¼©æ”¾ï¼ˆæ ‡å‡†åŒ–ã€å½’ä¸€åŒ–ï¼‰
    - ä¸»æˆåˆ†åˆ†æ (PCA)
    - åˆ†ç±»å˜é‡ç¼–ç ï¼ˆæ ‡ç­¾ç¼–ç ã€ç‹¬çƒ­ç¼–ç ï¼‰
    - è¿‡é‡‡æ · (Over Sampling)
    - é™é‡‡æ · (Under-sampling)
    - æ»‘åŠ¨çª—å£ (Sliding Window)
    - æ’å€¼ (Interpolation)
    - å¼‚å¸¸å€¼æ£€æµ‹å’Œå¤„ç†
    - ç‰¹å¾é€‰æ‹©
    - **ğŸ“š [é¢„å¤„ç†å®Œæ•´æŒ‡å—](examples/PREPROCESSING_GUIDE.md)**
    - **âš¡ [é¢„å¤„ç†é€ŸæŸ¥å¡](examples/PREPROCESSING_QUICK_REF.md)**
    - **ğŸ’¡ [10ä¸ªæ¨¡æ¿ç¤ºä¾‹](examples/preprocessing_templates.py)**

13. **æ¨¡å‹è¯„ä¼°** (`algorithms/utils/evaluation.py`)
    - å›å½’è¯„ä¼°æŒ‡æ ‡ï¼ˆMSEã€RMSEã€MAEã€RÂ²ï¼‰
    - åˆ†ç±»è¯„ä¼°æŒ‡æ ‡ï¼ˆå‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°ï¼‰
    - äº¤å‰éªŒè¯
    - å­¦ä¹ æ›²çº¿å’ŒéªŒè¯æ›²çº¿
    - æ¨¡å‹æ¯”è¾ƒ

## å¿«é€Ÿå¼€å§‹

### 1. å¤šå…ƒçº¿æ€§å›å½’ç¤ºä¾‹

```python
from algorithms.regression import MultipleLinearRegressionModel
import pandas as pd
from sklearn.model_selection import train_test_split

# åŠ è½½æ•°æ®
data = pd.read_csv('your_data.csv')
X = data.drop('target', axis=1)
y = data['target']

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# è®­ç»ƒæ¨¡å‹
model = MultipleLinearRegressionModel(normalize=True)
model.fit(X_train, y_train)

# é¢„æµ‹å’Œè¯„ä¼°
y_pred = model.predict(X_test)
metrics = model.evaluate(X_test, y_test)
print(metrics)

# æŸ¥çœ‹ç³»æ•°
print(model.get_coefficients())
```

### 2. éšæœºæ£®æ—ç¤ºä¾‹

```python
from algorithms.ensemble import RandomForestModel

# åˆ›å»ºå›å½’æ¨¡å‹
model = RandomForestModel(task_type='regression', n_estimators=100)
model.fit(X_train, y_train)

# è¯„ä¼°
metrics = model.evaluate(X_test, y_test)
print(metrics)

# æŸ¥çœ‹ç‰¹å¾é‡è¦æ€§
importance = model.get_feature_importance(top_n=10)
print(importance)

# ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§å›¾
model.plot_feature_importance(top_n=20)
```

### 3. é›†æˆå­¦ä¹ ç¤ºä¾‹

```python
from algorithms.ensemble import GradientBoostingModel, VotingEnsemble
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor

# æ¢¯åº¦æå‡
gb_model = GradientBoostingModel(task_type='regression', n_estimators=100)
gb_model.fit(X_train, y_train)

# æŠ•ç¥¨é›†æˆ
estimators = [
    ('lr', LinearRegression()),
    ('rf', RandomForestRegressor(n_estimators=50, random_state=42)),
    ('gb', gb_model.model)
]

voting_model = VotingEnsemble(estimators, task_type='regression')
voting_model.fit(X_train, y_train)
metrics = voting_model.evaluate(X_test, y_test)
```

### 4. æ•°æ®é¢„å¤„ç†ç¤ºä¾‹

```python
from algorithms.utils import DataPreprocessor

# åˆ›å»ºé¢„å¤„ç†å™¨
preprocessor = DataPreprocessor()

# 1. å¤„ç†ç¼ºå¤±å€¼ï¼ˆæ’å€¼ï¼‰
data_filled = preprocessor.interpolate_missing(data, method='linear')

# 2. ç¼–ç åˆ†ç±»å˜é‡
data_encoded = preprocessor.encode_categorical(data_filled, method='onehot')

# 3. ç‰¹å¾ç¼©æ”¾ï¼ˆæ ‡å‡†åŒ–ï¼‰
data_scaled = preprocessor.scale_features(data_encoded, method='standard')

# 4. PCAé™ç»´
data_pca = preprocessor.apply_pca(data_scaled, variance_threshold=0.95)

# 5. è¿‡é‡‡æ ·ï¼ˆè§£å†³ç±»åˆ«ä¸å¹³è¡¡ï¼‰
X_resampled, y_resampled = preprocessor.oversample(X, y)

# 6. æ»‘åŠ¨çª—å£ï¼ˆæ—¶é—´åºåˆ—ï¼‰
X_window, y_window = preprocessor.create_sliding_window(
    time_series_data, 
    window_size=7, 
    step=1
)

# 7. ç‰¹å¾é€‰æ‹©
X_selected, selected_features = preprocessor.select_features(
    X, y, k=10, task_type='regression'
)

# ç§»é™¤å¼‚å¸¸å€¼
data_clean = preprocessor.remove_outliers(data_scaled, method='iqr', threshold=1.5)

# æ•°æ®æ¢ç´¢
explorer = DataExplorer()
explorer.plot_correlation_matrix(data)
explorer.plot_distributions(data)
```

**ğŸ’¡ æŸ¥çœ‹å®Œæ•´çš„10ä¸ªé¢„å¤„ç†æ¨¡æ¿**: `python examples/preprocessing_templates.py`

### 5. æ¨¡å‹è¯„ä¼°å’Œæ¯”è¾ƒ

```python
from algorithms.utils import ModelEvaluator
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor

# åˆ›å»ºè¯„ä¼°å™¨
evaluator = ModelEvaluator()

# æ¯”è¾ƒå¤šä¸ªæ¨¡å‹
models = {
    'Linear Regression': LinearRegression(),
    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),
    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42)
}

comparison = evaluator.compare_models(models, X_train, y_train, cv=5, task_type='regression')
print(comparison)

# ç»˜åˆ¶å­¦ä¹ æ›²çº¿
evaluator.plot_learning_curve(models['Random Forest'], X_train, y_train)

# ç»˜åˆ¶éªŒè¯æ›²çº¿
param_range = [10, 50, 100, 200, 300]
evaluator.plot_validation_curve(
    RandomForestRegressor(random_state=42),
    X_train, y_train,
    param_name='n_estimators',
    param_range=param_range
)
```

## ä½¿ç”¨å»ºè®®

### ç¾èµ›Cé¢˜ï¼ˆæ•°æ®åˆ†æç±»ï¼‰

æ¨èç®—æ³•ï¼š
1. å¤šå…ƒçº¿æ€§å›å½’ - ç”¨äºå»ºç«‹åŸºå‡†æ¨¡å‹
2. éšæœºæ£®æ— - å¤„ç†éçº¿æ€§å…³ç³»
3. æ¢¯åº¦æå‡ - è·å¾—æ›´é«˜ç²¾åº¦
4. ç‰¹å¾é‡è¦æ€§åˆ†æ - è§£é‡Šæ¨¡å‹ç»“æœ

### ç¾èµ›Eé¢˜ï¼ˆç¯å¢ƒç§‘å­¦ç±»ï¼‰

æ¨èç®—æ³•ï¼š
1. æ—¶é—´åºåˆ—åˆ†æ + å›å½’
2. éšæœºæ£®æ— - å¤„ç†å¤æ‚ç¯å¢ƒå› ç´ 
3. é›†æˆå­¦ä¹  - æé«˜é¢„æµ‹ç¨³å®šæ€§
4. æ•°æ®é¢„å¤„ç† - å¤„ç†ç¼ºå¤±å’Œå¼‚å¸¸å€¼

### ç¾èµ›Fé¢˜ï¼ˆæ”¿ç­–ç±»ï¼‰

æ¨èç®—æ³•ï¼š
1. é€»è¾‘å›å½’ - åˆ†ç±»å’Œå†³ç­–åˆ†æ
2. å†³ç­–æ ‘ - å¯è§£é‡Šçš„å†³ç­–è§„åˆ™
3. SVM - é«˜ç»´æ•°æ®åˆ†ç±»
4. æ¨¡å‹è¯„ä¼° - éªŒè¯æ”¿ç­–æ•ˆæœ

## æ³¨æ„äº‹é¡¹

1. **æ•°æ®é¢„å¤„ç†**ï¼šä½¿ç”¨å‰åŠ¡å¿…å¯¹æ•°æ®è¿›è¡Œæ¸…æ´—å’Œé¢„å¤„ç†
2. **ç‰¹å¾å·¥ç¨‹**ï¼šæ ¹æ®é—®é¢˜ç‰¹ç‚¹æ„é€ åˆé€‚çš„ç‰¹å¾
3. **æ¨¡å‹é€‰æ‹©**ï¼šæ ¹æ®æ•°æ®è§„æ¨¡å’Œé—®é¢˜ç±»å‹é€‰æ‹©åˆé€‚çš„ç®—æ³•
4. **è¶…å‚æ•°è°ƒä¼˜**ï¼šä½¿ç”¨äº¤å‰éªŒè¯å’Œç½‘æ ¼æœç´¢ä¼˜åŒ–å‚æ•°
5. **æ¨¡å‹è§£é‡Š**ï¼šç¾èµ›éå¸¸é‡è§†æ¨¡å‹çš„å¯è§£é‡Šæ€§
6. **å¯è§†åŒ–**ï¼šä½¿ç”¨å›¾è¡¨å±•ç¤ºåˆ†æç»“æœ

## ç¾èµ›è®ºæ–‡å»ºæ¨¡æµç¨‹å»ºè®®

1. **é—®é¢˜åˆ†æ** â†’ ç¡®å®šä»»åŠ¡ç±»å‹ï¼ˆå›å½’/åˆ†ç±»ï¼‰
2. **æ•°æ®æ¢ç´¢** â†’ ä½¿ç”¨DataExplorerè¿›è¡Œåˆæ­¥åˆ†æ
3. **æ•°æ®é¢„å¤„ç†** â†’ ä½¿ç”¨DataPreprocessoræ¸…æ´—æ•°æ®
4. **ç‰¹å¾å·¥ç¨‹** â†’ æ„é€ å’Œé€‰æ‹©é‡è¦ç‰¹å¾
5. **æ¨¡å‹è®­ç»ƒ** â†’ å°è¯•å¤šä¸ªç®—æ³•å¹¶æ¯”è¾ƒ
6. **æ¨¡å‹ä¼˜åŒ–** â†’ è¶…å‚æ•°è°ƒä¼˜
7. **æ¨¡å‹è¯„ä¼°** â†’ ä½¿ç”¨ModelEvaluatorå…¨é¢è¯„ä¼°
8. **ç»“æœå¯è§†åŒ–** â†’ ç»˜åˆ¶å›¾è¡¨å±•ç¤ºç»“æœ
9. **æ•æ„Ÿæ€§åˆ†æ** â†’ éªŒè¯æ¨¡å‹ç¨³å®šæ€§
10. **æ’°å†™è®ºæ–‡** â†’ è¯¦ç»†è¯´æ˜å»ºæ¨¡è¿‡ç¨‹å’Œç»“æœ

## å‚è€ƒæ–‡çŒ®

- Scikit-learn Documentation: https://scikit-learn.org/
- XGBoost Documentation: https://xgboost.readthedocs.io/
- MCM/ICM Contest Archive: https://www.comap.com/

## è®¸å¯è¯

MIT License

## è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPull Requestï¼

## è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜ï¼Œè¯·é€šè¿‡GitHub Issuesè”ç³»ã€‚

---

**ç¥ç¾èµ›å–å¾—å¥½æˆç»©ï¼Good luck with MCM/ICM!**